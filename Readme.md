# **Data Augmentation, am√©liorer rapidement son mod√®le de Deep Learning** - [voir l'article](https://inside-machinelearning.com/data-augmentation-ameliorer-rapidement-son-modele-de-deep-learning/)

Dans ce **notebook** nous allons voir comment utiliser la **Data Augmentation** et le **Dropout** pour am√©liorer un mod√®le classique de **Deep Learning** pour la **classification d'images** !

Nous allons **utiliser notre mod√®le** pour classifier **des images de chiens et de chats** bien qu'il soit utilisable sur n'importe quel **jeu de donn√©es**.

Le **code complet** est disponible sur **GitHub** [√† cet adresse](https://github.com/tkeldenich/Image_Recognition_CNN_DataAugmentation.git).

L'**objectif** de ce notebook est de cr√©er un mod√®le de **Deep Learning** capable de rep√©rer sur une image un chien ou un chat. Le mod√®le aura seulement **deux types de r√©ponse** : 'chien' ou 'chat', c'est donc une **classification binaire**.

Nous allons prendre un **petit jeu de donn√©s** (4000 images) intentionnellement pour que le mod√®le ne soit pas optimal. Cela nous permettra de voir comment **am√©liorer notre mod√®le** malgr√© cette contrainte.

## **Entra√Æner le mod√®le CNN**

### **Charger les donn√©es**

Tout d'abord nous allons charger sur le notebook le **fichier zip** contenant les **images de chats et de chiens** dont nous avons besoin.

Ces images sont des donn√©es √† **3 dimensions**:
- **hauteur**
- **longueur**
- **couleur** (Rouge, Vert, Bleu)

O√π **chaque couleur** (RVB) est repr√©sent√© sur une **√©chelle de nuance de 0 √† 255**.

Le **fichier** contenant nos images **se trouve** sur [Github](https://github.com/tkeldenich/Image_Recognition_CNN_DataAugmentation.git)


```
!git clone https://github.com/tkeldenich/Image_Recognition_CNN_DataAugmentation.git
```

    Cloning into 'Image_Recognition_CNN_DataAugmentation'...
    remote: Enumerating objects: 4, done.[K
    remote: Counting objects: 100% (4/4), done.[K
    remote: Compressing objects: 100% (4/4), done.[K
    remote: Total 7 (delta 0), reused 4 (delta 0), pack-reused 3
    Unpacking objects: 100% (7/7), done.


On **d√©zip** le fichier :


```
from zipfile import ZipFile

with ZipFile('/content/Image_Recognition_CNN_DataAugmentation/images.zip', 'r') as zipObj:
   zipObj.extractall('images')
```

Ce fichier contient **plusieurs sous-fichiers** hi√©rarchis√©s:
- **train**, pour l'entra√Ænement du mod√®le
- **validation**, pour v√©rifer si le mod√®le est overfit
- **test**, pour tester le mod√®le

Chacun de ces fichiers contient un **r√©pertoire cats** (chats) et un **r√©pertoire dogs** (chiens).

Cela permet de savoir directement √† **quel label** (chat ou chien) **une image fait r√©f√©rence**. Ces r√©pertoires seront utiles notamment **lors de la labellisation** faite dans la partie **G√©n√©rateur & Preprocessing**.

Pour **chaque fichier**, on cr√©e une **variable** contenant le **chemin du fichier**.


```
train_dir = 'images/train/'
validation_dir = 'images/validation/'
test_dir = 'images/test/'

train_cats_dir = 'images/train/cats'
train_dogs_dir = 'images/train/dogs'
validation_cats_dir = 'images/validation/cats'
validation_dogs_dir = 'images/validation/dogs'
test_cats_dir = 'images/test/cats'
test_dogs_dir = 'images/test/dogs'
```

On peut par la suite afficher **la nombre d'images de chaque fichier** pour voir si l'on en a bien 4000.


```
import os

print('total training cat images:', len(os.listdir(train_cats_dir)))
print('total training dog images:', len(os.listdir(train_dogs_dir)))
print('total validation cat images:', len(os.listdir(validation_cats_dir)))
print('total validation dog images:', len(os.listdir(validation_dogs_dir)))
print('total test cat images:', len(os.listdir(test_cats_dir)))
print('total test dog images:', len(os.listdir(test_dogs_dir)))
```

    total training cat images: 1000
    total training dog images: 1000
    total validation cat images: 500
    total validation dog images: 500
    total test cat images: 500
    total test dog images: 500


## **Les g√©n√©rateurs**

Une fois que les donn√©es sont **charg√©s dans notre environnement** nous allons les **ins√©rer dans des variables**... mais pas n'importe quelles variables.. des **g√©n√©rateurs !**

En fait **les g√©n√©rateurs** sont des **fonctions dans lequel on stocke des variables**, des √©l√©ments ou encore des images.

**L'avantage des g√©n√©rateurs** c'est qu'ils ne calculent pas la valeur de chaque √©l√©ment. En effet, **ils calculent les √©l√©ments uniquement lorsqu'on leur demande de le faire**. C'est ce qu'on appelle une **√©valuation paresseuse** (**lazy evaluation**).

Cette **√©valuation paresseuse** est utile lorsque l'on a un **tr√®s grand nombre de donn√©es** √† calculer. Elle permet d'**utiliser imm√©diatement les donn√©es d√©j√† calcul√©es**, pendant que le reste des donn√©es est en cours de calcul.

Les g√©n√©rateurs permettent donc un **gain de rapidit√© et d'espace m√©moire !**

Cette m√©thode est particuli√®rement **pratique** pour l'entra√Ænement d'un **mod√®le de Deep Learning** qui fonctionne sur des **lots de donn√©e**(batch). Les lots sont charg√©s seulement **lorsque le mod√®le en a besoin** (par it√©ration).

Dans notre cas, on va utiliser la fonction *ImageDataGenerator* qui permet d'**initialiser des g√©n√©rateurs Python** pour charger des images.

*ImageDataGenerator* permet aussi de **redimensionner les valeurs RVB**.

C'est une m√©thode id√©al qui permet au mod√®le d'√™tre plus pr√©cis. Nous allons donc **redimensionner** cette √©chelle de 0 √† 255 en **0 √† 1**.



```
from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(rescale=1./255)
```

## **Preprocessing**

On a **initialis√© nos g√©n√©rateurs**, maintenant il faut leur **indiquer o√π chercher nos images** !

Pour cela, on utilise la fonction *flow_from_directory*  qui permet de charger des **lots de donn√©es** depuis un chemin de fichier.

Cette fonction a de nombreux param√®tres qui nous **facilite vraiment la t√¢che**, on peut remercier ceux qui l'ont cod√© !

Les **param√®tres** que l'on utilise :
- **le chemin du fichier**, que l'on a d√©j√† stock√© dans des variables plus t√¥t

- **batch_size**, la taille du lot d'images √† charger. Ici on choisit des lots de **20 images**.

- **target_size**, le g√©n√©rateur peut redimensionner automatiquement la hauteur et la largeur des images charg√©s. On choisit une dimension petite de **150x150** pour que le mod√®le s'entra√Æne plus rapidement

- **class_mode**, ce param√®tre permet de choisir le type de labellisation que l'on veut. Dans notre cas, nous voulons d√©tecter le label 'chien' ou 'chat'. Soit l'un, soit l'autre. C'est donc une classification binaire, on utilise donc **'binary'** !

Ces **quatres param√®tres nous suffisent** mais il en existe bien d'autre si l'on veut **pousser davantage la configuration** de nos g√©n√©rateurs, [n'h√©sitez pas √† regarder la documentation pour en apprendre plus !](https://keras.io/api/preprocessing/image/)


```
train_generator = datagen.flow_from_directory(train_dir,
                                                    batch_size=20,
                                                    target_size=(150, 150),
                                                    class_mode='binary')

validation_generator = datagen.flow_from_directory(validation_dir,
                                                        batch_size=20,
                                                        target_size=(150, 150),
                                                        class_mode='binary')
```

    Found 2000 images belonging to 2 classes.
    Found 1000 images belonging to 2 classes.


On peut **v√©rifier** que les lots d'images cr√©√©s **correspondent** aux dimensions voulu en utilisant la fonction *shape*.


```
for data_batch, labels_batch in train_generator:
  print('data batch shape:', data_batch.shape)
  print('labels batch shape:', labels_batch.shape)
  break
```

    data batch shape: (20, 150, 150, 3)
    labels batch shape: (20,)


On a bien des **lots de 20 images** de **dimensions 150x150x3** et des **lots de 20 labels**, 'chien' ou 'chat', **un pour chaque image**. 

## **Construire le mod√®le**

Nos **donn√©es sont pr√©trait√©s**, nous pouvons maintenant **construire notre mod√®le !**

On travaille **avec des images**, nous allons donc utilis√© un mod√®le √† **couches de convolution** comme expliqu√© dans [cet article](https://inside-machinelearning.com/cnn-couche-de-convolution/).

Le **nombre de couches** d√©pend de la **complexit√© du probl√®me** et de la **taille des images**. Ici, quatre couches *MaxPooling* sont suffisantes.

Reste √† savoir quelle **fonction d'activation** utiliser. Il suffit de se r√©f√©rer √† [cet article](https://inside-machinelearning.com/fonction-dactivation-comment-ca-marche-une-explication-simple/#Quelle_fonction_pour_quel_cas) pour **savoir rapidement** quelle fonction est **pertinente**.

Dans notre cas, on utilise la fonction *sigmoid* dans la **derni√®re couche** de notre mod√®le pour faire de la **classification binaire**.


```
from keras import layers
from keras import models

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',
                        input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```

Ensuite, nous pouvons **compiler notre mod√®le**, configuer son apprentissage.

On choisit comme **loss function** 'binary_crossentropy', comme **optimisateur** RMSprop et la m√©trique 'acc' pour accuracy (pr√©cision).

Nous n'avons pas encore √©crit d'**articles** aux sujets de ces fonctions mais **√ßa ne saurait tarder !** :)


```
from keras import optimizers

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics=['acc'])
```

## **Entra√Æner le mod√®le**

On **entra√Æne le mod√®le** avec la fonction *fit_generator()*, c'est le m√™me principe que la fonction *fit()* mais adapt√© aux **g√©n√©rateurs**.

On choisit **ces param√®tres** pour l'entra√Ænement :
- le **chemin du fichier** contenant les donn√©es d'entra√Ænement
- le **nombre d'epoch**, le nombre de fois que le mod√®le repasse sur l'ensemble de donn√©es
- **steps_per_epoch**, est en fait similaire a batch_size, ici 100 steps_per_epoch correspond √† un batch_size de 20 (nombre de donn√©es d'entra√Ænement divis√© par nombre de steps_per_epoch)
- le chemin du fichier contenant les **donn√©es de validation** (pour √©valuer l'overfitting du mod√®le)
- **validation_steps**, c'est le nombre de steps_per_epoch apr√®s lequel le mod√®le se test sur les donn√©es de validation

On peut r√©sumer ces param√®tres ainsi :  le mod√®le **s'entra√Æne 30 fois sur l'ensemble de donn√©es**. Chaque fois qu'il s'entra√Æne, les **donn√©es sont divis√©s en 100 lots** (batch) sur lesquels il se focalise un √† un. √Ä chaque fois qu'il a visit√© **50 lots**, il **teste son apprentissage** sur l'ensemble des donn√©es de validation.

*Pour cet entra√Ænement il est conseill√© d'avoir un GPU, sinon d'utiliser le GPU int√©gr√© √† Google Colab (gratuit).*





```
history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)
```

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
      warnings.warn('`Model.fit_generator` is deprecated and '


    Epoch 1/30
    100/100 [==============================] - 16s 89ms/step - loss: 0.6960 - acc: 0.5290 - val_loss: 0.6782 - val_acc: 0.5560
    Epoch 2/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.6634 - acc: 0.5933 - val_loss: 0.6440 - val_acc: 0.6200
    Epoch 3/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.6101 - acc: 0.6596 - val_loss: 0.6179 - val_acc: 0.6560
    Epoch 4/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.5628 - acc: 0.7232 - val_loss: 0.6025 - val_acc: 0.6820
    Epoch 5/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.5363 - acc: 0.7270 - val_loss: 0.6104 - val_acc: 0.6790
    Epoch 6/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.5093 - acc: 0.7466 - val_loss: 0.6383 - val_acc: 0.6550
    Epoch 7/30
    100/100 [==============================] - 9s 85ms/step - loss: 0.4729 - acc: 0.7650 - val_loss: 0.6070 - val_acc: 0.6810
    Epoch 8/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.4688 - acc: 0.7670 - val_loss: 0.5576 - val_acc: 0.7040
    Epoch 9/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.4234 - acc: 0.8070 - val_loss: 0.5587 - val_acc: 0.7010
    Epoch 10/30
    100/100 [==============================] - 9s 88ms/step - loss: 0.4181 - acc: 0.8066 - val_loss: 0.5590 - val_acc: 0.7120
    Epoch 11/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.3891 - acc: 0.8204 - val_loss: 0.5741 - val_acc: 0.7130
    Epoch 12/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.3394 - acc: 0.8639 - val_loss: 0.6314 - val_acc: 0.6920
    Epoch 13/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.3240 - acc: 0.8689 - val_loss: 0.5645 - val_acc: 0.7170
    Epoch 14/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.2972 - acc: 0.8699 - val_loss: 0.5638 - val_acc: 0.7180
    Epoch 15/30
    100/100 [==============================] - 8s 84ms/step - loss: 0.2768 - acc: 0.8906 - val_loss: 0.6583 - val_acc: 0.7120
    Epoch 16/30
    100/100 [==============================] - 8s 84ms/step - loss: 0.2516 - acc: 0.8917 - val_loss: 0.6025 - val_acc: 0.7170
    Epoch 17/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.2331 - acc: 0.9111 - val_loss: 0.6362 - val_acc: 0.7280
    Epoch 18/30
    100/100 [==============================] - 9s 85ms/step - loss: 0.1803 - acc: 0.9355 - val_loss: 0.6260 - val_acc: 0.7220
    Epoch 19/30
    100/100 [==============================] - 9s 85ms/step - loss: 0.1831 - acc: 0.9362 - val_loss: 0.6333 - val_acc: 0.7160
    Epoch 20/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.1546 - acc: 0.9504 - val_loss: 0.6558 - val_acc: 0.7260
    Epoch 21/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.1231 - acc: 0.9630 - val_loss: 0.7241 - val_acc: 0.7120
    Epoch 22/30
    100/100 [==============================] - 8s 85ms/step - loss: 0.1166 - acc: 0.9649 - val_loss: 0.8492 - val_acc: 0.7040
    Epoch 23/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.0981 - acc: 0.9714 - val_loss: 0.8671 - val_acc: 0.7070
    Epoch 24/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.0884 - acc: 0.9791 - val_loss: 1.0019 - val_acc: 0.7080
    Epoch 25/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.0770 - acc: 0.9741 - val_loss: 0.8980 - val_acc: 0.7160
    Epoch 26/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.0726 - acc: 0.9761 - val_loss: 0.8597 - val_acc: 0.7260
    Epoch 27/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.0635 - acc: 0.9827 - val_loss: 1.1542 - val_acc: 0.7080
    Epoch 28/30
    100/100 [==============================] - 9s 86ms/step - loss: 0.0512 - acc: 0.9861 - val_loss: 0.9447 - val_acc: 0.7110
    Epoch 29/30
    100/100 [==============================] - 9s 87ms/step - loss: 0.0317 - acc: 0.9935 - val_loss: 0.9879 - val_acc: 0.7120
    Epoch 30/30
    100/100 [==============================] - 9s 88ms/step - loss: 0.0333 - acc: 0.9931 - val_loss: 0.9835 - val_acc: 0.7190


On ne l'a pas vu pr√©c√©demment, mais il est toujours conseill√© de **sauvegarder notre mod√®le apr√®s l'apprentissage**, cela permet d'√©viter de recommencer l'entra√Ænement √† chaque nouvelle session.

Vous pouvez le **sauvegarder facilement** avec la fonction *save()* et lors d'une **nouvelle session** vous pourrez le **charger** avec la fonction *load_model()*.


```
model.save('model_trained.h5')

#model = model.load_model('model_trained.h5')
```

## **√âvaluer le mod√®le**

On **√©value notre mod√®le** en comparant les **m√©triques**, les courbes de **loss** et les courbes de **pr√©cision**.


```
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()
```


    
![png](Readme_files/Readme_33_0.png)
    



    
![png](Readme_files/Readme_33_1.png)
    


Sur les **donn√©es d'entra√Ænement**, le mod√®le est **de plus en plus performant** mais sur les **donn√©es de validation** il stagne, voir devient **de moins en moins bon**.

Le mod√®le **se sp√©cialise sur les donn√©es d'entra√Ænement** mais n'est plus capable d'**interpr√©ter** les donn√©es de validations (et les donn√©es r√©elles en g√©n√©ral), il est en **overfitting**.

Essayons notre mod√®le sur les **donn√©es de test** :


```
test_generator = datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')

model.evaluate(test_generator)
```

    Found 1000 images belonging to 2 classes.
    50/50 [==============================] - 3s 52ms/step - loss: 0.9935 - acc: 0.7390

    [0.9935357570648193, 0.7390000224113464]



**La pr√©cision est de 0.74**, les perfomances sont bonnes mais pas exceptionnelles.

**Pas de soucis n√©anmoins, on peut am√©liorer notre mod√®le !**

# **Contourner l'overfitting**

## **Data Augmentation**

### **Qu'est-ce que c'est ?**

L'**overfitting** est d√ª au fait que l'on dispose de trop **peu de donn√©es**.

Le mod√®le a trop **peu de cas d'usage diff√©rent** ce qui rend difficile son entra√Ænement. Par cons√©quent , il ne peut pas **d√©velopper des r√®gles** de d√©cisions pouvant √™tre **g√©n√©ralis√© √† de nouvelles donn√©es**.

Il nous faut **plus de donn√©es**... et donc dans notre cas, **plus de photos**. Ce n'est pas une t√¢che facile a faire.

**Heureusement**, il existe une **technique** pour augmenter notre nombre de donn√©es. On peut cr√©er des **images artificielles** gr√¢ce √† la **Data Augmentation** (litt√©ralement : l'augmentation des donn√©es).

L'id√©e derri√®re la **Data Augmentation** est de reproduire les donn√©es pr√©existantes en leur appliquant une **transformation al√©atoire**. Par exemple, appliquer un **effet mirroir** sur une image.

Lors de l'entra√Ænement, notre mod√®le apprendra sur **beaucoup plus de donn√©es** tout en ne rencontrant **jamais deux fois la m√™me image**.

Le mod√®le est donc **expos√© √† davantage de donn√©es**. Cela lui permet de **mieux g√©n√©raliser**.

**V√©rifions cela maintenant !** ;)

### **Comment l'utiliser ?**

On peut faire de la **Data Augmentation** en utilisant la fonction *ImageDataGenerator* d√©j√† utilis√© plus haut et en **modifiant certains param√®tres** :

- **rotation_range** pour faire pivoter une image de fa√ßon al√©atoire sur une plage entre 0 et la valeur choisis (maximum 180 degr√©)
- **width_shift** et **height_shift** sont des plages (en fraction de la largeur ou de la hauteur totale) √† l'int√©rieur desquelles on peut redimensionner al√©atoirement des images verticalement ou horizontalement.
- **shear_range** est une plage permettant de rogner(d√©couper) de mani√®re al√©atoire l'image
- **zoom_range** permet de zoomer de mani√®re al√©atoire √† l'int√©rieur des images
- **horizontal_flip** retourne horizontalement des images de mani√®re al√©atoire (certaines seront retourner d'autres non)
- **fill_mode** est la strat√©gie utilis√©e pour remplir les pixels nouvellement cr√©√©s, qui peuvent appara√Ætre apr√®s un pivotage, un rognage, etc

On utilise **ces quelques param√®tres** mais encore une fois, il en existe d'autres sur [la documentation Keras !](https://keras.io/api/preprocessing/image/)


```
augmented_datagen = ImageDataGenerator(
      rotation_range=40,
      width_shift_range=0.2,
      height_shift_range=0.2,
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')
```

Ensuite, on peut **afficher ces nouvelles images** pour voir concr√©tement ce qu'a produit notre **Data Augmentation**.


```
from keras.preprocessing import image

fnames = [os.path.join(train_cats_dir, fname) for
     fname in os.listdir(train_cats_dir)]

img_path = fnames[4]
img = image.load_img(img_path, target_size=(150, 150))

x = image.img_to_array(img)

x = x.reshape((1,) + x.shape)

i=0
fig = plt.figure(figsize=(7,7))

for batch in augmented_datagen.flow(x, batch_size=1):
    ax = fig.add_subplot(2,2,i+1)
    ax.imshow(image.array_to_img(batch[0]))
    i += 1
    if i % 4 == 0:
        break

plt.show()
```


    
![png](Readme_files/Readme_46_0.png)
    


**Pour visualiser les images augment√©es**, on a appliqu√© le **param√®tre** *fill_mode*.

Pour l'**entra√Ænement du mod√®le** il n'est pas n√©cessaire de l'utiliser. On initialise donc un **g√©n√©rateur** sans ce param√®tre.


```
augmented_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,)
```

La **Data Augmentation** concerne seulement les **donn√©es d'entra√Ænement**... pour les autres donn√©es on initialise un **g√©n√©rateur simple** !


```
datagen = ImageDataGenerator(rescale=1./255)
```

Comme pour le premier entra√Ænement, on indique **le chemin des fichiers contenant nos images** aux deux g√©n√©rateurs et on **configure le preprocessing** a effectuer.


```
train_generator = augmented_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')

validation_generator = datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')
```

    Found 2000 images belonging to 2 classes.
    Found 1000 images belonging to 2 classes.


Une √©tape de **v√©rification des dimensions**, toujours utile:


```
for data_batch, labels_batch in train_generator:
  print('data batch shape:', data_batch.shape)
  print('labels batch shape:', labels_batch.shape)
  break
```

    data batch shape: (20, 150, 150, 3)
    labels batch shape: (20,)


## **Dropout**

Le **mod√®le** ne verra jamais deux fois la m√™me donn√©e mais certaines images qu'il voit sont **fortement similaires**. Les donn√©es sont corr√©l√©es car elles proviennent d'un petit nombre d'images de base. Avec la **Data Augmentation** on ne peut pas produire de **nouvelles informations**, on peut seulement remanier, **remixer des informations existantes**.

Cela peut ne pas √™tre suffisant pour se d√©barrasser compl√®tement de l'**overfitting**. On va donc utilisez une deuxi√®me technique: le **Dropout**.

Dans un mod√®le de **Deep Learning**, chaque couche  du r√©seau apprend lors de la **phase d'entra√Ænement** du mod√®le.

Lors de cet entra√Ænement, **des poids sont associ√©s aux neurones** de chaque couche.

Ces **poids repr√©sentent l'apprentissage** du mod√®le. Plus ils sont √©lev√©s, **plus le neurone a d'influence** sur la donn√©e.

L'id√©e du **Dropout** est de **supprimer al√©atoirement certains de ces neurones** et donc supprimer les poids associ√©s. Cela peut para√Ætre paradoxale mais cette technique permet d'**am√©liorer l'apprentissage du mod√®le**.

En √©cartant certains neurones **lors de l'apprentissage**, les autres **neurones** sont contraint de **se surpasser** pour que la couche donne de bon r√©sultats.

En fait, on ajoute du **bruit** pendant l'apprentissage du mod√®le ce qui a pour effet de **contraindre les neurones restant √† √™tre plus robuste** et donc plus performant. **Le r√©seaux de neurones s'adapte** √† la situation !

Le *Dropout* est une **couche** a part enti√®re. L'id√©e est de l'utiliser apr√®s avoir configur√© les **couches convolutionnelles** (MaxPooling2D et Flatten).


```
model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu',
input_shape=(150, 150, 3)))

model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())

model.add(layers.Dropout(0.5))

model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
optimizer=optimizers.RMSprop(lr=1e-4),
metrics=['acc'])
```

## **Entra√Æner le nouveau mod√®le**

On **entra√Æne notre mod√®le** avec la fonction *fit_generator()* mais cette fois avec 100 epochs car on sait que le mod√®le ne va pas overfitter rapidement gr√¢ce √† la **Data Augmentation** et au **Dropout**.


```
history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=50)
```

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
      warnings.warn('`Model.fit_generator` is deprecated and '


    Epoch 1/100
    100/100 [==============================] - 18s 175ms/step - loss: 0.6948 - acc: 0.5098 - val_loss: 0.6860 - val_acc: 0.5000
    Epoch 2/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.6874 - acc: 0.5483 - val_loss: 0.6783 - val_acc: 0.5410
    Epoch 3/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.6716 - acc: 0.5713 - val_loss: 0.6944 - val_acc: 0.5400
    Epoch 4/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.6542 - acc: 0.6146 - val_loss: 0.6315 - val_acc: 0.6170
    Epoch 5/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.6404 - acc: 0.6247 - val_loss: 0.6199 - val_acc: 0.6410
    Epoch 6/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.6126 - acc: 0.6618 - val_loss: 0.6106 - val_acc: 0.6590
    Epoch 7/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.6179 - acc: 0.6474 - val_loss: 0.6230 - val_acc: 0.6300
    Epoch 8/100
    100/100 [==============================] - 17s 171ms/step - loss: 0.6099 - acc: 0.6708 - val_loss: 0.5857 - val_acc: 0.6900
    Epoch 9/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.6049 - acc: 0.6716 - val_loss: 0.5757 - val_acc: 0.6860
    Epoch 10/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.5799 - acc: 0.6827 - val_loss: 0.6113 - val_acc: 0.6400
    Epoch 11/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.5919 - acc: 0.6708 - val_loss: 0.5573 - val_acc: 0.6920
    Epoch 12/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.5746 - acc: 0.6887 - val_loss: 0.5525 - val_acc: 0.7040
    Epoch 13/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.5716 - acc: 0.7105 - val_loss: 0.5425 - val_acc: 0.7230
    Epoch 14/100
    100/100 [==============================] - 17s 172ms/step - loss: 0.5661 - acc: 0.7087 - val_loss: 0.6049 - val_acc: 0.6890
    Epoch 15/100
    100/100 [==============================] - 18s 175ms/step - loss: 0.5807 - acc: 0.6922 - val_loss: 0.5330 - val_acc: 0.7200
    Epoch 16/100
    100/100 [==============================] - 18s 176ms/step - loss: 0.5561 - acc: 0.7102 - val_loss: 0.5859 - val_acc: 0.6870
    Epoch 17/100
    100/100 [==============================] - 17s 173ms/step - loss: 0.5743 - acc: 0.7021 - val_loss: 0.5598 - val_acc: 0.6980
    Epoch 18/100
    100/100 [==============================] - 17s 171ms/step - loss: 0.5438 - acc: 0.7309 - val_loss: 0.5297 - val_acc: 0.7190
    Epoch 19/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.5569 - acc: 0.6983 - val_loss: 0.5681 - val_acc: 0.6960
    Epoch 20/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.5550 - acc: 0.7246 - val_loss: 0.5281 - val_acc: 0.7260
    Epoch 21/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.5591 - acc: 0.7193 - val_loss: 0.5427 - val_acc: 0.7090
    Epoch 22/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.5424 - acc: 0.7364 - val_loss: 0.5320 - val_acc: 0.7320
    Epoch 23/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.5243 - acc: 0.7338 - val_loss: 0.5952 - val_acc: 0.7000
    Epoch 24/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.5416 - acc: 0.7138 - val_loss: 0.5919 - val_acc: 0.6980
    Epoch 25/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.5459 - acc: 0.7109 - val_loss: 0.5271 - val_acc: 0.7240
    Epoch 26/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.5170 - acc: 0.7362 - val_loss: 0.5097 - val_acc: 0.7450
    Epoch 27/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.5210 - acc: 0.7463 - val_loss: 0.5221 - val_acc: 0.7380
    Epoch 28/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.5405 - acc: 0.7261 - val_loss: 0.5096 - val_acc: 0.7400
    Epoch 29/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.5054 - acc: 0.7491 - val_loss: 0.5128 - val_acc: 0.7380
    Epoch 30/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.5200 - acc: 0.7540 - val_loss: 0.4871 - val_acc: 0.7520
    Epoch 31/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.5174 - acc: 0.7346 - val_loss: 0.5468 - val_acc: 0.7350
    Epoch 32/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4971 - acc: 0.7546 - val_loss: 0.5284 - val_acc: 0.7300
    Epoch 33/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4965 - acc: 0.7519 - val_loss: 0.5473 - val_acc: 0.7280
    Epoch 34/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4713 - acc: 0.7636 - val_loss: 0.5241 - val_acc: 0.7340
    Epoch 35/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4973 - acc: 0.7494 - val_loss: 0.4973 - val_acc: 0.7460
    Epoch 36/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.4976 - acc: 0.7593 - val_loss: 0.5120 - val_acc: 0.7540
    Epoch 37/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.4902 - acc: 0.7546 - val_loss: 0.6443 - val_acc: 0.6900
    Epoch 38/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4933 - acc: 0.7581 - val_loss: 0.5043 - val_acc: 0.7530
    Epoch 39/100
    100/100 [==============================] - 17s 174ms/step - loss: 0.4897 - acc: 0.7729 - val_loss: 0.4884 - val_acc: 0.7620
    Epoch 40/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.4825 - acc: 0.7650 - val_loss: 0.4651 - val_acc: 0.7770
    Epoch 41/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4857 - acc: 0.7643 - val_loss: 0.4796 - val_acc: 0.7620
    Epoch 42/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4885 - acc: 0.7739 - val_loss: 0.5113 - val_acc: 0.7470
    Epoch 43/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4975 - acc: 0.7608 - val_loss: 0.5039 - val_acc: 0.7640
    Epoch 44/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4768 - acc: 0.7692 - val_loss: 0.5911 - val_acc: 0.7390
    Epoch 45/100
    100/100 [==============================] - 17s 165ms/step - loss: 0.4393 - acc: 0.8072 - val_loss: 0.4582 - val_acc: 0.7850
    Epoch 46/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4692 - acc: 0.7658 - val_loss: 0.4972 - val_acc: 0.7530
    Epoch 47/100
    100/100 [==============================] - 17s 165ms/step - loss: 0.4693 - acc: 0.7712 - val_loss: 0.5065 - val_acc: 0.7580
    Epoch 48/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.5027 - acc: 0.7614 - val_loss: 0.4898 - val_acc: 0.7570
    Epoch 49/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.4764 - acc: 0.7785 - val_loss: 0.4772 - val_acc: 0.7680
    Epoch 50/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4625 - acc: 0.7764 - val_loss: 0.4783 - val_acc: 0.7780
    Epoch 51/100
    100/100 [==============================] - 17s 169ms/step - loss: 0.4577 - acc: 0.7769 - val_loss: 0.4620 - val_acc: 0.7830
    Epoch 52/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4625 - acc: 0.7850 - val_loss: 0.4702 - val_acc: 0.7760
    Epoch 53/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4598 - acc: 0.7770 - val_loss: 0.4723 - val_acc: 0.7840
    Epoch 54/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4694 - acc: 0.7663 - val_loss: 0.4317 - val_acc: 0.7930
    Epoch 55/100
    100/100 [==============================] - 17s 170ms/step - loss: 0.4385 - acc: 0.7903 - val_loss: 0.4668 - val_acc: 0.7730
    Epoch 56/100
    100/100 [==============================] - 17s 165ms/step - loss: 0.4355 - acc: 0.7971 - val_loss: 0.4748 - val_acc: 0.7740
    Epoch 57/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4587 - acc: 0.7962 - val_loss: 0.4647 - val_acc: 0.7880
    Epoch 58/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4831 - acc: 0.7618 - val_loss: 0.4626 - val_acc: 0.7870
    Epoch 59/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4309 - acc: 0.7965 - val_loss: 0.4535 - val_acc: 0.7910
    Epoch 60/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4210 - acc: 0.7938 - val_loss: 0.4740 - val_acc: 0.7790
    Epoch 61/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4392 - acc: 0.7986 - val_loss: 0.5019 - val_acc: 0.7790
    Epoch 62/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4229 - acc: 0.8065 - val_loss: 1.2120 - val_acc: 0.6330
    Epoch 63/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4409 - acc: 0.7912 - val_loss: 0.4553 - val_acc: 0.7840
    Epoch 64/100
    100/100 [==============================] - 17s 171ms/step - loss: 0.4608 - acc: 0.7753 - val_loss: 0.5393 - val_acc: 0.7480
    Epoch 65/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4661 - acc: 0.7731 - val_loss: 0.5699 - val_acc: 0.7300
    Epoch 66/100
    100/100 [==============================] - 16s 164ms/step - loss: 0.4511 - acc: 0.7764 - val_loss: 0.4358 - val_acc: 0.7920
    Epoch 67/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4401 - acc: 0.7953 - val_loss: 0.4471 - val_acc: 0.7930
    Epoch 68/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4122 - acc: 0.8089 - val_loss: 0.4226 - val_acc: 0.8150
    Epoch 69/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4223 - acc: 0.7889 - val_loss: 0.4371 - val_acc: 0.7960
    Epoch 70/100
    100/100 [==============================] - 16s 165ms/step - loss: 0.4375 - acc: 0.7980 - val_loss: 0.4820 - val_acc: 0.7620
    Epoch 71/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4271 - acc: 0.8042 - val_loss: 0.4357 - val_acc: 0.8040
    Epoch 72/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4455 - acc: 0.7838 - val_loss: 0.5511 - val_acc: 0.7450
    Epoch 73/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4450 - acc: 0.7923 - val_loss: 0.5421 - val_acc: 0.7530
    Epoch 74/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4088 - acc: 0.8093 - val_loss: 0.4677 - val_acc: 0.7900
    Epoch 75/100
    100/100 [==============================] - 16s 165ms/step - loss: 0.4034 - acc: 0.8146 - val_loss: 0.4593 - val_acc: 0.7770
    Epoch 76/100
    100/100 [==============================] - 16s 165ms/step - loss: 0.4211 - acc: 0.8175 - val_loss: 0.4461 - val_acc: 0.7920
    Epoch 77/100
    100/100 [==============================] - 17s 165ms/step - loss: 0.4141 - acc: 0.8034 - val_loss: 0.4724 - val_acc: 0.7830
    Epoch 78/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4156 - acc: 0.8050 - val_loss: 0.4772 - val_acc: 0.7790
    Epoch 79/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4214 - acc: 0.8082 - val_loss: 0.5425 - val_acc: 0.7290
    Epoch 80/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4315 - acc: 0.7977 - val_loss: 0.4581 - val_acc: 0.7870
    Epoch 81/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4345 - acc: 0.7960 - val_loss: 0.4670 - val_acc: 0.7840
    Epoch 82/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4326 - acc: 0.8046 - val_loss: 0.4228 - val_acc: 0.8090
    Epoch 83/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4003 - acc: 0.8228 - val_loss: 0.4398 - val_acc: 0.8030
    Epoch 84/100
    100/100 [==============================] - 17s 165ms/step - loss: 0.4163 - acc: 0.8073 - val_loss: 0.4426 - val_acc: 0.8060
    Epoch 85/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4099 - acc: 0.8145 - val_loss: 0.4107 - val_acc: 0.8140
    Epoch 86/100
    100/100 [==============================] - 16s 165ms/step - loss: 0.4141 - acc: 0.8107 - val_loss: 0.4680 - val_acc: 0.7970
    Epoch 87/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4171 - acc: 0.8062 - val_loss: 0.4611 - val_acc: 0.7780
    Epoch 88/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4086 - acc: 0.8021 - val_loss: 0.4209 - val_acc: 0.8210
    Epoch 89/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.4022 - acc: 0.8161 - val_loss: 0.4566 - val_acc: 0.7840
    Epoch 90/100
    100/100 [==============================] - 17s 165ms/step - loss: 0.3865 - acc: 0.8319 - val_loss: 0.5238 - val_acc: 0.7580
    Epoch 91/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.3780 - acc: 0.8274 - val_loss: 0.4144 - val_acc: 0.8220
    Epoch 92/100
    100/100 [==============================] - 17s 167ms/step - loss: 0.3955 - acc: 0.8228 - val_loss: 0.4243 - val_acc: 0.8170
    Epoch 93/100
    100/100 [==============================] - 17s 168ms/step - loss: 0.4115 - acc: 0.8151 - val_loss: 0.4522 - val_acc: 0.7990
    Epoch 94/100
    100/100 [==============================] - 16s 165ms/step - loss: 0.3986 - acc: 0.8145 - val_loss: 0.4535 - val_acc: 0.8000
    Epoch 95/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4149 - acc: 0.8062 - val_loss: 0.4264 - val_acc: 0.8130
    Epoch 96/100
    100/100 [==============================] - 17s 165ms/step - loss: 0.3711 - acc: 0.8383 - val_loss: 0.4897 - val_acc: 0.8050
    Epoch 97/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.3904 - acc: 0.8190 - val_loss: 0.4435 - val_acc: 0.8070
    Epoch 98/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4195 - acc: 0.8011 - val_loss: 0.4555 - val_acc: 0.7890
    Epoch 99/100
    100/100 [==============================] - 17s 166ms/step - loss: 0.4112 - acc: 0.8094 - val_loss: 0.4642 - val_acc: 0.7880
    Epoch 100/100
    100/100 [==============================] - 16s 165ms/step - loss: 0.3940 - acc: 0.8231 - val_loss: 0.4544 - val_acc: 0.7980


Ensuite, on **sauvegarde** notre **nouveau mod√®le**.


```
model.save('model_trained_enhanced.h5')

#model = model.load_model('model_trained_enhanced.h5')
```

Et on trace les **courbes de pr√©cision et de loss** pour v√©rifier si le mod√®le **overfit** !


```
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()
```


    
![png](Readme_files/Readme_64_0.png)
    



    
![png](Readme_files/Readme_64_1.png)
    


Le mod√®le overfit beaucoup moins et beaucoups moins vite, nos deux techniques ont donc bien march√© ! :)

On v√©rifie cela sur nos **donn√©es de test** :


```
test_generator = datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')

model.evaluate(test_generator)
```

    Found 1000 images belonging to 2 classes.
    50/50 [==============================] - 3s 51ms/step - loss: 0.4608 - acc: 0.7950

    [0.46075958013534546, 0.7950000166893005]



Une **perte de 0.46** et une **pr√©cision 0.79**, on a r√©ussi a **am√©liorer notre mod√®le**.

Il est **toujours possible de faire mieux** alors n'h√©sitez pas √† reprendre le code et le **modifier par vous-m√™me** !
